{
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "182ace665ffdfe628e8de82169ced2bbbe57809d4b4c299761a22648602bf416",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "1db5bb5a73d9182aa94ac0e66dd6bf5e62c20151f7ce75aa0f219cb9ce3547e7",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 6144,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=8, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 8,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "57fcb270c8888aff7e6a164c2a43b7114633967d4070f656fbcf907fec0ed8ad",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 8,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 20480,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "45275d98294e3cacd049ae4673f3e2450b3bfc5b28945221ef0d99a07e984c23",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 6144,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "98d0fbdaab8b3b6c328701397d74958db07d21e0e8e4c0d1e98be6820f607e5c",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "ac38dbd5d595a0d43e5e204a26c5b15737fdf9715e657597f3889575c18aaecb",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 20480,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "1cc53c6126487207af4b493ca45dc4898bf0a3aef5a2307421df7c7ddf0f90c7",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 20480,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "a63d739a1963adfbfb6121a2971884d91c6668aade7ec0f53242cbe49dfd7178",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=128, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 128,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "ab27799e546657ec0d85de55fcfe4a13815b96065ea274a8947d46dea2b41097",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 9216,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "7d42e8b2b048c3ecc82018debec9a541c6e4ac2b2154c0aca31f4e641b9a8234",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1024,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "c1b7f0f896db3ebc75441968aa92eb7f83d0a6257466c35d5f158a09d17da71f",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=128, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 128,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "ced61da431a270b988463fd365adec1237de26568e07475924928430a4c63d64",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 9216,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "f4aaf5db17893850d69759d023497572655a453f75f7c5bd6688fb5a9c3c4182",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "305ba2fde204573de48af2674fcf47bacd533ae6842220bd44ea599e0c38a8e1",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1024,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "0e46a71f07525725fa20d7601147918777cf19b7daa0a45ac0ff5511193c99ac",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "31c6f19e3242afb144fda984646a48b341a2247b5dd8833bc3e492d18b7b69fa",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "a652a9025df9ce14a0f83e80c8b4a015655073ba93731b0af31929812ad19f15",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 4096,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "01afb5f67487c2fe07cd735cd3979d36380105edc60ed825e3d5e2f29910f585",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "016b747849588e20d0631aa8c1c44a0f50de586f0f2b085659822e12a330eafc",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 4096,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "c1b7f0f896db3ebc75441968aa92eb7f83d0a6257466c35d5f158a09d17da71f",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "8f54eff6befe8e70d8f70751744c9e7d396e9b9b3505182a689e18cea5203ad1",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "af12b32930e43e0f27a455e8952935afbeda0abc02095bbe603c1106f61158f9",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "357e86cebb32c668fef60ceef95c912bb954935ed855b4d75adad4b886202677",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 20480,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "8c9df12f0542589fa6bd3b441310f69960973df1ded8ee2fd7e4dac53fe5580c",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1536,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "ee0a2138e77831a88f4990ab58997f4bc54b14254699f53f36a4786ba255254d",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "7b3cecdcef30522da65ea8b80b75babd5a17f4e360ca0bb6931cfa0ea27a1c7a",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1536,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "7d42e8b2b048c3ecc82018debec9a541c6e4ac2b2154c0aca31f4e641b9a8234",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1024,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=32, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 32,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "977fbe7128891fc3ff6fcb16675cb1db3c5b7f55c8e0bcfd2c247a80fad75fa9",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 6144,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "305ba2fde204573de48af2674fcf47bacd533ae6842220bd44ea599e0c38a8e1",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1024,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "bb93aecc74084b596d4c1a916e9bad83f151c18ec3760b1f1a3f0fd369b61cc4",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "44b3722b3d6cbb4304a7661c431f60dd569d2a83dfea55f7811b1db3d963e55e",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "7b3cecdcef30522da65ea8b80b75babd5a17f4e360ca0bb6931cfa0ea27a1c7a",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1536,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "bd4fe45e01e29b5d29da4ed73cfab25db1ca7087c0466490ab6b893cfefd6dc9",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "d5e4da0e9b183dd9d82bf53817c7d73e71a13e7191d0487ec8e359817a0544fe",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "f7912e541c3cc87bc9645757f7f6fa6b24f3e7a91796b034fceb17e22bdceb07",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "31c6f19e3242afb144fda984646a48b341a2247b5dd8833bc3e492d18b7b69fa",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "0e46a71f07525725fa20d7601147918777cf19b7daa0a45ac0ff5511193c99ac",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=128, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 128,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "ab27799e546657ec0d85de55fcfe4a13815b96065ea274a8947d46dea2b41097",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 9216,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "44b3722b3d6cbb4304a7661c431f60dd569d2a83dfea55f7811b1db3d963e55e",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "bd4fe45e01e29b5d29da4ed73cfab25db1ca7087c0466490ab6b893cfefd6dc9",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=32, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 32,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "d0f05e78f1a8e0464431f828130bdf26efe959f96ab3b84c570fd9e59534c399",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "01afb5f67487c2fe07cd735cd3979d36380105edc60ed825e3d5e2f29910f585",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "8c9df12f0542589fa6bd3b441310f69960973df1ded8ee2fd7e4dac53fe5580c",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1536,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "af12b32930e43e0f27a455e8952935afbeda0abc02095bbe603c1106f61158f9",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "ee5cef2ffc8338e2d5025009ec419fbfd12629c0f9c182ee8ebc5690923af90d",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "8f54eff6befe8e70d8f70751744c9e7d396e9b9b3505182a689e18cea5203ad1",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "58e45f6f020ecd91a7d2fcac784a75e0a5b4f68d8ca4d071c7685ab5a9d512b8",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "6aa0a9626838f755c60dfbf6d74d969bc94a494c381c7be1fe75720b20328835",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "ca9bab8cccb0030de1c94e778b475a45693fec9c38f56c525dc88e7a0d8778bd",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "bcd9b67a336fbbfd5795fe79ea6e9270c837d7c5c8f70905de7570d3ce57ca76",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "878ef0a116619f7926d798a4f456a93b2e70f2a04c68b9648c2497ef59715c7d",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "45079f01b1a81e802adbf52ce49057768f201d9dc13f2b3eb87b04c73a6e2c59",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "ca9bab8cccb0030de1c94e778b475a45693fec9c38f56c525dc88e7a0d8778bd",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "fac7024b760b3937410a2f6d8df82450fc730fd0e11a33b36ccd620e8f86ded8",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "878ef0a116619f7926d798a4f456a93b2e70f2a04c68b9648c2497ef59715c7d",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "c759fad2b9807d7bc22ed0e134cde1d25f685ad791b494d709b9732eec1254d3",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=32, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 32,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "5a84226700df59fc9cf836ee599b3e4313979e2d100cf1c2771421f52267e5ad",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "adce2e7a2e9b807c59482c9098aebb8b24ef161b93cb94d50194aa39be14be68",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "239f65a28e94043e2274443cad5fc05a3af6900d399d8af976eecdbeca48f863",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1024,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "59cf16bf730db40990c5dc35293ff02496f30332fe477f9ae54402d301f4ce5b",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "6ce901ab21662743638b82fae25599c6ff2892ef69efeacfe6d2d323294ea2fe",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1024,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "d06b0e92ab41149ec0e27f010d828ae9688a66afa32d9faea386ff126b6f4bbd",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "c759fad2b9807d7bc22ed0e134cde1d25f685ad791b494d709b9732eec1254d3",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "45febdeba0a5ebe9a0f424119fdd6c28a18433e4bd724d0fa54bbad74545692e",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "8f534447e57333aefc21cf5593ef0ba77fccfd6f58a59762e99945d134bc5b3d",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "250f5eeac39e3be0bf3e448762228a0364a36d90df2beb6650e5d58bf78ca476",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "6f097887fee676045cd9135029ee0b99f34bba9d739d2f5a4d021b16066153b3",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=128, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 128,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "6a10dfa812276ea1f89d71d7f71054623b33d0c6424b4db19ae3e1efb72f26dc",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 9216,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "5ded54b18a84e2de7e86bac38624bb40ca382f52f3d2414454d71283bda14ce1",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=8, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 8,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "ceebf735660111212b48511cb04126e8f9137d066a559d64d87a0a8cf7909c1a",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 8,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 20480,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "98e5db7d308cfc91f12cbe531207ecedc792c3c7801c4e5610a1ff127adf936c",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1536,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "7260c9d31e8acb72be7e4426fa899bb545bde74d2e919856915ff87fbd1a1c50",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1536,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=128, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 128,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "ba66e92e1e4541afa6dea429167f7bf2ef06bce5248f83d8218134509a392d83",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 9216,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=128, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 128,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "6a10dfa812276ea1f89d71d7f71054623b33d0c6424b4db19ae3e1efb72f26dc",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 9216,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "00a72fc093ad9842892b7c43283fb099b59929991f2efbc6517cc3c7b8084622",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 20480,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "98b3b4d6bb175a7127b4e6f5bd9aacf361fc6bd29994955837fbb42f3000909c",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 20480,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "98e5db7d308cfc91f12cbe531207ecedc792c3c7801c4e5610a1ff127adf936c",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1536,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "59cf16bf730db40990c5dc35293ff02496f30332fe477f9ae54402d301f4ce5b",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "adce2e7a2e9b807c59482c9098aebb8b24ef161b93cb94d50194aa39be14be68",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "7260c9d31e8acb72be7e4426fa899bb545bde74d2e919856915ff87fbd1a1c50",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1536,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "6b03d240c123dfeec84f0696ec1ad9e6e3c5e36281ff20e26b3505bebf94c106",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "bed0e430b801b3ff6dfbd94ae4f51d844635229eb79ead38c2ee54a08db5b2a5",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "afee8c572c697ee42679224afe7a5f6b21603c7c0fe2ef4ad7ed1008f411cb15",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 6144,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=32, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 32,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "d8ffa6dfa2bbf42f8e6ed5e9a53eab2d4cbbea11a6cf19083ae0ffb0b0da79e4",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 6144,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "4d78611633598dd9e2082f4f456d56396e93d1e37e751b1b9386e4f248cba636",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "10b2c661eaab6887d0113f7563d488ffd370c63b5ef3fd6ea5557fd73165a073",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 6144,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "63f5834d314953a73ab998f26dad182292872dff6230ab9ba2ad1bdab84a8e60",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 4096,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "de031ff31675650431a412c615ed4c9bae856beb5d3245d3d490e982d2c79cdb",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "250f5eeac39e3be0bf3e448762228a0364a36d90df2beb6650e5d58bf78ca476",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "ac72019cf77446ad2509cea47ba9b3f5d273bf6ef8a6726a933480a921cb0bac",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 4096,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "deccd243575f30614d93957703c6d237a81fb79669e894a4bd0612a3a50a9161",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 20480,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "f4b252380af7a725583fe04f21bca813bcf09b3ded8066662dfeb8e2155b2517",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "bcd9b67a336fbbfd5795fe79ea6e9270c837d7c5c8f70905de7570d3ce57ca76",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "239f65a28e94043e2274443cad5fc05a3af6900d399d8af976eecdbeca48f863",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1024,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "45079f01b1a81e802adbf52ce49057768f201d9dc13f2b3eb87b04c73a6e2c59",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=1, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 1,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "6ce901ab21662743638b82fae25599c6ff2892ef69efeacfe6d2d323294ea2fe",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1024,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "8a14a3ed4eba77b0fc17afc79a47838f0f5e9614764048c369aad74cbc6cb047",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "14cf6954257e0ca0bf1e2f474a475f7a0801970b90bf2bf174b4fe7446a9502c",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "0430434dca78d30640ad18d26ba602a6971425ba0196519e2b1e97b2e103f499",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "51702633654cc0109b03ba0f4e73354948dc387f0676dadd15e6ab4d474430cb",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "824957b2412a8f555e9f8a266208889e55dbb793df388cf33715f17c93309186",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "c14408af99ccde495146e492a4301e9dcccace31ab6540400bf122cb198c56e6",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=32, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 32,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "5ea1399a6c0f182a182add57fb13867b031da5d4351500103369ad24eb15f28c",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 6144,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "d11b184e7443364294a07ebf35af8a76a33ee48e82a45dae8a12284852a47639",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=128, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 128,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "6d940f8b2acd52af506e86608445cdf379d3b8b80deb94dfe9f9707c1087cf91",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 9216,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "d663688635ec7212b22038489a1680507678d408acb3c5d705e7638b1eb9b3ec",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "937f88c63213045c407b351082067730c1b264a2595bdd5d8f55f9ee62ab11e3",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1536,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=128, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 128,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "79aeafd810107921ad7d01d0d335a52a603d77c846661cc47647b9095a2bd36c",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 9216,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "5102d603e4152c91607b134f4ba6256e84356b45f28cefea1ca9a2dbbc155e81",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "4d517b46361dce1df2901e10f78188a414ae1ba14fcfa6b5da61c3d08ceaf87d",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "b5cb008626296c636952f6d04ef56647e91b5450f8471e25076c1214816fe676",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1536,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "04a0cc1fec0e2abe2bde15164447944d9832afab034a845744ca55a5817d93cb",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=8, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 8,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "d33ab954d8a1b8ec332771435f2dbaea8696db20121ef4591c4e1047d02d4b8c",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 8,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 20480,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "ca94807200091c03dc75f061773ab9de9238ffb1bfcfa5ee299b4c6ee7863297",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "89ee5fe94b03af7aaa9bd2b80f541485b7bc5b6c6e079f9a20666b310c468c4c",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1024,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "10e1eb2167af59ed1a0a139b71dcbfcc0df1c5083fb987a4802534fcaa41090f",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "543fda14220735ee04508f33ee90d746bb80c28a18a9fd56f67e289cf7fffb95",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "1c5dfd5f57bd0ca0becfe51f105a3266ce0e588d43c494ac7a26634b668c8749",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1024,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "62718eb67bdb864d2c671103bb28ac272ddbe87030823cf9f124f0e9e6f94f85",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 20480,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "5bbb3b6d376c63c668c4ed50f70a58e26351e8d35979e8e739b1ada89a9251e3",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "b5cb008626296c636952f6d04ef56647e91b5450f8471e25076c1214816fe676",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1536,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=32, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 32,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "671ab3e415d6691130c4dfc8f41c878ec22713e0602c6ba19ed0672f7471af44",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "937f88c63213045c407b351082067730c1b264a2595bdd5d8f55f9ee62ab11e3",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1536,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "2e150e206cfaa1285a02e155235202208421e27c6246077825b5252763b3cff1",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 20480,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "79c00e2f41ada00fc4a8031ca8156bf32427eff0b5d4c07787c9c6c9e9fc55a4",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 20480,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "2ff4cf2a0bbd6151ce60ffe9aff0687d9ced2b11fbc2e2af050de72987df8f2d",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "d2fbbe803b68a86c002c2152227ea8a7cfbcd2cdcce0c7020073988973ce9233",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "4aaf16d5dc474f33e2bec5be0c74f4ebcbdf0796f872d118f050cce6a9a43753",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 4096,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "04a0cc1fec0e2abe2bde15164447944d9832afab034a845744ca55a5817d93cb",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "6d7697de520405a3a7f6d2d572844e0887ca106ce81403042cf6520e8f9921d0",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 4096,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=128, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 128,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "79aeafd810107921ad7d01d0d335a52a603d77c846661cc47647b9095a2bd36c",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 9216,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "10e1eb2167af59ed1a0a139b71dcbfcc0df1c5083fb987a4802534fcaa41090f",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "321047231233e8e47aac8cc2bc9152caef6ad0d2de5c0af7df005a563988f078",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 6144,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "ca94807200091c03dc75f061773ab9de9238ffb1bfcfa5ee299b4c6ee7863297",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "a5a9fe88960964b8cac50d1c77872de3722f94e3f846682d32f8a9d22704ab71",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 6144,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "89b36ac21ca994bea2d8daa4492afeeb742d8d2e18540a567aefc273a43b7d90",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "2ff4cf2a0bbd6151ce60ffe9aff0687d9ced2b11fbc2e2af050de72987df8f2d",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "14cf6954257e0ca0bf1e2f474a475f7a0801970b90bf2bf174b4fe7446a9502c",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "51702633654cc0109b03ba0f4e73354948dc387f0676dadd15e6ab4d474430cb",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "89ee5fe94b03af7aaa9bd2b80f541485b7bc5b6c6e079f9a20666b310c468c4c",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1024,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "d2fbbe803b68a86c002c2152227ea8a7cfbcd2cdcce0c7020073988973ce9233",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "1c5dfd5f57bd0ca0becfe51f105a3266ce0e588d43c494ac7a26634b668c8749",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1024,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "8a14a3ed4eba77b0fc17afc79a47838f0f5e9614764048c369aad74cbc6cb047",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=True, div_dp=True, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": true,
            "div_dp": true,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "5ad0878cb5ede2a810b4747c58320c5fed29138723c1b0f4eb9be53c8d338d99",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "221ba6adb736c47c76ffb3f83e2764ceb64aa04aad2d1f3b254a44a9fd810e4d",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "af84df6f298cbfe63f19ad4c3e6c222898c66591f09541cad4fb1bce8d49eaff",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "165a213bad3b18a4c0b236e061a6135cdf7862515f3a3a6e4b6e48a912ad42ac",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "07bdd307ab4eb6e89d97f1ac73298766b46b2a509ca68eedf710ef3c3efaafc6",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 4096,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "2ba8064f1ac6a6a5ec9254587a97bed7ae42a147310758ce768d6f70d5fe2ec4",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=128, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 128,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "eeb972c63d791164e0a195857fbfa714ac8ccf70fa27ef8e503c73c1defa05b6",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 9216,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "6a467bb71930df1912f9c21073ee555e977701c902a57e9852d28473ac9859cb",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "7ff229d10c67a6632f60aa64c80f8b3ff69ea792cf0e51c204368c5d01adb8e5",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "d053a5dab02f3e19be7dbcef11e209f0c25ac0cba52a5bde382d390829619b58",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "680444bd985e2acbfca8255e7bb36ba0ddf548a1e86bd99bec5c15f147a00050",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "0b70113e097afcb64d4e39e30def1ddf02cc00bdcc9e9a598754048b63bf8641",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1536,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "9bc8f77db793e6ded9b1d4c4af51353ddd94cc1e91e5f9fe315e0f55af154537",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "645362f533a21762857d15d9d70014653a584b9e1d6ff5fbb4770c7004e7806e",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1536,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "f3b61f05a17b4b73ef99b93368b98fe67ee3b86b5295af53adc6ab50a02ffdf7",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1024,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "3314431bef6a15b8d44400cdbd068cb8cb03a821abb296bea62ea303eaa794a7",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "8c96787f21392cbfbc689b5073711e1381e6e833d534b78d2d4d75f5e3389ab7",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1024,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "b60a0102b9e65ff441f3f129de67b21e4603529427642c714653ebded1bb2c73",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=32, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 32,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "fa9364371a3db410e8e755d532987071c822ef5a4159aa77e47ef6a12387387c",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 6144,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "0b70113e097afcb64d4e39e30def1ddf02cc00bdcc9e9a598754048b63bf8641",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1536,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "3314431bef6a15b8d44400cdbd068cb8cb03a821abb296bea62ea303eaa794a7",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "de72f935b671b725460fa87a5203316384eaf4a09f4a43970c4fc55f8869f769",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 6144,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "645362f533a21762857d15d9d70014653a584b9e1d6ff5fbb4770c7004e7806e",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1536,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "b60a0102b9e65ff441f3f129de67b21e4603529427642c714653ebded1bb2c73",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 2048,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "eaccfbf9a04a1013af14b3c19045f5f39ea774a1535ef9622fb423b71c62bd90",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 6144,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "6f31b2e00ce3f7228165875c6f64a8eea4319e0525dbeabd39377763f115f8aa",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "c6ef6f9c3bdf1bc4b70849f93805da80aa74800c115b6a7e07a647f6aee5edd4",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "af84df6f298cbfe63f19ad4c3e6c222898c66591f09541cad4fb1bce8d49eaff",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=128, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 128,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "8c3d7bc6e074fd95a03cfcb0dd34593ba8ae83278cf134c6318b4284039e6869",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 9216,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "f3b61f05a17b4b73ef99b93368b98fe67ee3b86b5295af53adc6ab50a02ffdf7",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1024,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "de50dd5d695eaad4c3de280b83471be270e2677cbfbdb53597a4ca5ba508134e",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "d9d6cefa2e15eccbcc4c6817edd48e4adfff8bc11ce75b0fddf3fd3c5dcee419",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 20480,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "8c96787f21392cbfbc689b5073711e1381e6e833d534b78d2d4d75f5e3389ab7",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 1024,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=128, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 128,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "eeb972c63d791164e0a195857fbfa714ac8ccf70fa27ef8e503c73c1defa05b6",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 9216,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "c6ef6f9c3bdf1bc4b70849f93805da80aa74800c115b6a7e07a647f6aee5edd4",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp16', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp16",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "78879071344684b5c9213ee226cef39a0b2f1981d3dd1b10caa04538de66f36e",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 5120,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "9bc8f77db793e6ded9b1d4c4af51353ddd94cc1e91e5f9fe315e0f55af154537",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "d053a5dab02f3e19be7dbcef11e209f0c25ac0cba52a5bde382d390829619b58",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=32, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 32,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "0fa5a8e2cb3d152d4eca199671fa48b82b089ab9bf1c10162e07a6cbd1922723",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "f34be62916c762a89ac37f60e34a3e1c39cdc96138e3cc4789f7e2c0b46e8f56",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "9e0dd36646c4f58d3c0de239bade657d8e6315e2ecf9acae4e531e37ea0c042c",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 20480,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "fdc777c2e496d1993acb98b47010c6154f964076c44c2659eda4e8d6a4b98730",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 20480,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "2ba8064f1ac6a6a5ec9254587a97bed7ae42a147310758ce768d6f70d5fe2ec4",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=16, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 16,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "477270a4deda9a79db591c21d0a201124f86d363a16d310e7e2d8ccefb6f7ca0",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 3072,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=128, BLOCK_K=32, BLOCK_Dp=32, num_warps=8, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 128,
            "BLOCK_K": 32,
            "BLOCK_Dp": 32,
            "num_warps": 8,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "cb01de86416e075010d7c00e44efb5c7c2b708e969439f7561c34fac619a02cf",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 8,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 20480,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=89).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 89
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm89",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "d3861d12c53ea03dec6e1bf846c3a13a2188af1b18f5a73283cc1d67adce9a8a",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e4nv",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=32, BLOCK_K=16, BLOCK_Dp=32, num_warps=2, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 32,
            "BLOCK_K": 16,
            "BLOCK_Dp": 32,
            "num_warps": 2,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "d72467c0fa946eaf690feab10b8cfc7fc3a5b2dc610f76d67a3c9b31813ac201",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 2,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 4096,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=32, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp32', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 32,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp32",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "a6d278dbebd09afa15a0a7248ba645367dea80939c71068371e4ee6007a6d714",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    },
    "Config(BLOCK_N=16, BLOCK_K=64, BLOCK_Dp=64, num_warps=4, num_stages=2, parallel_k=2, div_k=False, div_d=False, div_dp=False, acc_dtype='fp32', dtype='fp16', sm=86).ptx": {
        "config": {
            "BLOCK_N": 16,
            "BLOCK_K": 64,
            "BLOCK_Dp": 64,
            "num_warps": 4,
            "num_stages": 2,
            "parallel_k": 2,
            "div_k": false,
            "div_d": false,
            "div_dp": false,
            "acc_dtype": "fp32",
            "dtype": "fp16",
            "sm": 86
        },
        "meta": {
            "allowed_dot_input_precisions": [
                "tf32",
                "tf32x3",
                "ieee"
            ],
            "arch": "sm86",
            "backend_name": "cuda",
            "cluster_dims": [
                1,
                1,
                1
            ],
            "debug": false,
            "default_dot_input_precision": "tf32",
            "deprecated_fp8_dtypes": [],
            "enable_fp_fusion": true,
            "extern_libs": [
                [
                    "libdevice",
                    "/home/allan/anaconda3/envs/ml/lib/python3.10/site-packages/triton/backends/nvidia/lib/libdevice.10.bc"
                ]
            ],
            "global_scratch_align": 1,
            "global_scratch_size": 0,
            "hash": "6a467bb71930df1912f9c21073ee555e977701c902a57e9852d28473ac9859cb",
            "launch_cooperative_grid": false,
            "max_num_imprecise_acc_default": 0,
            "maxnreg": null,
            "name": "implicit_conv3d_kernel",
            "num_buffers_warp_spec": 0,
            "num_consumer_groups": 0,
            "num_ctas": 1,
            "num_stages": 2,
            "num_warps": 4,
            "ptx_version": 82,
            "reg_dec_producer": 0,
            "reg_inc_consumer": 0,
            "sanitize_overflow": true,
            "shared": 10240,
            "supported_fp8_dtypes": [
                "fp8e4b15",
                "fp8e5"
            ],
            "target": null,
            "tmem_size": 0,
            "triton_version": "3.3.1"
        }
    }
}